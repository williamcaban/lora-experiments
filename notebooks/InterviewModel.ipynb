{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f514695-a942-43a8-9dd2-cc9e891fede3",
   "metadata": {},
   "source": [
    "# Interview a Model\n",
    "By William Caban\n",
    "\n",
    "Interview a model for analogous harmony palette.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f3cf1-8ce9-45b4-9df1-99adc8505595",
   "metadata": {},
   "source": [
    "## Interview model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b83e5ec-19f7-463e-b651-34e23f018417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import generate, load\n",
    "from tqdm.notebook import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cada2268-61cd-4165-a79f-965adf0833a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBAL CONFIG PARAMETERS\n",
    "\n",
    "# Specify the maximum number of tokens\n",
    "max_tokens = 1_000\n",
    "\n",
    "# Specify if tokens and timing information will be printed\n",
    "verbose = False\n",
    "\n",
    "# Some optional arguments for causal language model generation\n",
    "generation_args = {\n",
    "    \"temp\": 0.1,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"repetition_context_size\": 20,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "# System template\n",
    "_system = \"You are a cautious assistant. You are an expert in color palette. \" + \\\n",
    "          \"Given a Color generate a JSON array of 10 colors for an analogous harmony palette.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22163421-1e0d-43b3-9905-1a110e70a151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f9ea3b1f37471fa31ddf4a0f9075d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Specify the model checkpoint\n",
    "checkpoint = \"instructlab/granite-7b-lab\"\n",
    "\n",
    "# Load the corresponding model and tokenizer\n",
    "model0, tokenizer0 = load(path_or_hf_repo=checkpoint, tokenizer_config={'legacy': 'false'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202ed9d2-95be-43c9-b24b-2ddd7decf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hex_list(text):\n",
    "    \"\"\"\n",
    "    return a list of up to 10 hex color numbers in the original text\n",
    "    \"\"\"\n",
    "    hexapattern = r'(#[0-9a-fA-F]+)'\n",
    "    hex_list = re.findall(hexapattern, text)\n",
    "    # silently drop any invalid hex entry in the form '#123456'\n",
    "    condition = lambda x: len(x) != 7\n",
    "    hex_list_clean = [x for x in hex_list if not condition(x)]\n",
    "    # only return the first 10 entries (workaround response including more colors than expected)\n",
    "    return hex_list_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3eeb56f-131b-4b3b-8054-45ccda33522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_prompt(color, tokenizer):\n",
    "    global _system\n",
    "    # Specify the prompt and conversation history\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\" : _system},\n",
    "        {\"role\": \"user\", \"content\": f\"Color: {color}\" },\n",
    "        {\"role\": \"assistant\", \"content\": \"\" }\n",
    "    ]\n",
    "\n",
    "    # Transform the prompt into the chat template\n",
    "    return tokenizer.apply_chat_template(\n",
    "        conversation=conversation, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c5a12f-b021-486e-82fb-c4edf338ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(prompt, model, tokenizer, DEBUG=False):\n",
    "    global max_tokens\n",
    "    global verbose\n",
    "    global generation_args\n",
    "    # Generate a response with the specified settings\n",
    "    response = generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        verbose=verbose,\n",
    "        **generation_args,\n",
    "    )\n",
    "    \n",
    "    if DEBUG is True:\n",
    "        print(f\"Answer: {response}\")\n",
    "        \n",
    "    return extract_hex_list(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbbaab0f-e4a6-43d0-9fd9-7940951a53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview(df_set, model_cname, model, tokenizer):\n",
    "    df = df_set.copy()\n",
    "    \n",
    "    if model_cname not in df.columns:\n",
    "        # make sure the column exist for continuation logic\n",
    "        df[model_cname]=pd.NA\n",
    "    \n",
    "    df[model_cname]=pd.NA\n",
    "    for indx, row in tqdm(df.iterrows(), desc=f\"Interviewing {model_cname}\"):\n",
    "        prompt = to_prompt(row['input'], tokenizer)\n",
    "    \n",
    "        # only invoke the llm if there is no answer with this model\n",
    "        try:\n",
    "            if df.isnull().loc[indx, model_cname].sum() > 0:\n",
    "                df.loc[indx, model_cname] = str(query(prompt, model, tokenizer))\n",
    "            else:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"ERROR: {e}\\n df.loc results: {df.isnull().loc[indx, model_cname]} with count={df.isnull().loc[indx, model_cname].sum()}\")\n",
    "            sys.exit()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab49eea3-e07f-4284-964d-d9511b5812f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function extract_hex_list in module __main__:\n",
      "\n",
      "extract_hex_list(text)\n",
      "    return a list of up to 10 hex color numbers in the original text\n",
      "\n",
      "Help on function to_prompt in module __main__:\n",
      "\n",
      "to_prompt(color, tokenizer)\n",
      "\n",
      "Help on function query in module __main__:\n",
      "\n",
      "query(prompt, model, tokenizer, DEBUG=False)\n",
      "\n",
      "Help on function interview in module __main__:\n",
      "\n",
      "interview(df_set, model_cname, model, tokenizer)\n",
      "\n",
      "None None None None\n"
     ]
    }
   ],
   "source": [
    "print(f\"{help(extract_hex_list)} {help(to_prompt)} {help(query)} {help(interview)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
